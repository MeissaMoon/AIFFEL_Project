{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "searching-tokyo",
   "metadata": {},
   "source": [
    "### Segmentation\n",
    "---\n",
    "### 세그먼테이션 종류  \n",
    "![세그](https://aiffelstaticprd.blob.core.windows.net/media/original_images/semantic_vs_instance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-johns",
   "metadata": {},
   "source": [
    "#### 1) 시맨틱 세그멘테이션(Semantic Segmentation)\n",
    "- 클래스에 따른 시맨틱 세그멘테이션 맵(semantic segmentation map)\n",
    "- [Segnet Demo](https://mi.eng.cam.ac.uk/projects/segnet/#demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-palmer",
   "metadata": {},
   "source": [
    "#### 2) 인스턴스 세그멘테이션(Instance Segmentation)\n",
    "![is](https://aiffelstaticprd.blob.core.windows.net/media/images/mask-rcnn-head.max-800x600.png)\n",
    "- RoIAlign, 클래스별 마스크 분리를 통해 클래스별 Object Detection과 시멘틱 세그멘테이션을 하나의 Task로 엮어낸 것으로 평가받는 중요한 모델인 \n",
    "  Mask R-CNN을 사용\n",
    "- Mask R-CNN은 Faster R-CNN에서 특성 추출방식을 \"RoIAlign\" 방식으로 개선을 하고 세그멘테이션을 더한 방식\n",
    "- Faster R-CNN과 비교한 구조도를 보면 U-Net처럼 피처 맵(feature map)의 크기를 키워 마스크(mask)를 생성해내는 부분을 통해 인스턴스에 해당하는 영역(인스턴스 맵)을 추론. Mask R-CNN은 클래스에 따른 마스크를 예측할 때, 여러 가지 태스크를 한 모델로 학습하여 물체 검출의 성능을 높임\n",
    "- Bounding box regression을 하는 Bbox head와 마스크를 예측하는 Mask Head로 나뉨.\n",
    "- Mask map의 경우 시맨틱 세그멘테이션과 달리 상대적으로 작은 28x28의 특성 맵 크기를 가짐.\n",
    "- RoIAlign을 통해 줄어든 특성에서 마스크를 예측하기 때문에 사용하려는 목적에 따라서 정확한 마스크를 얻으려는 경우에는 부적합할 수 있음\n",
    "- [Understanding Region of Interest — (RoI Align and RoI Warp)](https://towardsdatascience.com/understanding-region-of-interest-part-2-roi-align-and-roi-warp-f795196fc193)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-graduate",
   "metadata": {},
   "source": [
    "### 주요 세그멘테이션 모델 (1) FCN\n",
    "#### 참고자료\n",
    "- [Fully Convolutional Networks for Semantic Segmentation - 허다운](https://www.youtube.com/watch?v=_52dopGu3Cw)\n",
    "- [FCN 논문 리뷰 — Fully Convolutional Networks for Semantic Segmentation](https://medium.com/@msmapark2/fcn-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-fully-convolutional-networks-for-semantic-segmentation-81f016d76204)\n",
    "  \n",
    "  \n",
    "![FCN](https://aiffelstaticprd.blob.core.windows.net/media/images/fcn.max-800x600.png)\n",
    "![FCN2](https://aiffelstaticprd.blob.core.windows.net/media/images/fcn_2.max-800x600.png)\n",
    "\n",
    "#### fully connected layer 대신 CNN을 붙여주는 이유\n",
    "  \n",
    "CNN은 이미지 내 위치의 특성을 유지하지만 fully connected layer는 위치를 고려하지 않는다. 위치정보를 유지하면서 클래스 단위의 히트맵(heat map)을 얻어 세그멘테이션을 하기 위해, fully connected layer를 CNN으로 대체합니다.\n",
    "  \n",
    "마지막 CNN의 세팅은 위치의 특성을 유지하면서 이미지 분류를 하기 위해서 마지막 CNN은 1x1의 커널 크기(kernel size)와 클래스의 개수만큼의 채널을 갖습니다. 이렇게 CNN을 거치면 클래스 히트맵을 얻을 수 있습니다.\n",
    "  \n",
    "그러나 히트맵의 크기는 일반적으로 CNN과 pooling 레이어를 거치면서 크기가 줄었기 때문에 원본 이미지보다 작습니다. 이를 키워주는 방법을 upsampling 이라고 합니다. Upsampling에는 여러 가지 방법이 있습니다. 그중 FCN에서는 Deconvolution과 Interpolation 방식을 활용합니다. Deconvolution은 컨볼루션 연산을 거꾸로 해준 것이라고 볼 수 있습니다. Interpolation은 보간법으로 주어진 값들을 통해 추정해야 하는 픽셀(여기서는 특성 맵의 크기가 커지면서 메꾸어야 하는 중간 픽셀들을 의미합니다.) 추정하는 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-breakdown",
   "metadata": {},
   "source": [
    "#### Upsampling  \n",
    "![업샘플링](https://aiffelstaticprd.blob.core.windows.net/media/images/fcn_3.max-800x600.png)\n",
    "![FCN32](https://aiffelstaticprd.blob.core.windows.net/media/images/fcn_4.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-software",
   "metadata": {},
   "source": [
    "### 주요 세그멘테이션 모델 (2) U-Net\n",
    "#### 참고자료  \n",
    "- [딥러닝논문읽기모임의 U-Net: Convolutional Networks for Biomedical Image Segmentation](https://www.youtube.com/watch?v=evPZI9B2LvQ)\n",
    "- [U-Net 논문 리뷰 — U-Net: Convolutional Networks for Biomedical Image Segmentation](https://medium.com/@msmapark2/u-net-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-u-net-convolutional-networks-for-biomedical-image-segmentation-456d6901b28a)\n",
    "  \n",
    "![ss](https://aiffelstaticprd.blob.core.windows.net/media/images/u-net.max-800x600.png)  \n",
    "우측의 Contracting path는 일반적으로 우리가 사용해왔던 Convolution network와 유사한 구조를 가집니다. 각 블록은 두개의 3x3 convolution 계층과 ReLu를 가지고 그 뒤로 downsampling을 위해서 2x2의 커널을 2 stride로 max pooling을 하게 됩니다. Downsampling을 거친 후 다음 convolution의 채널 크기는 두 배씩 늘어나도록 설계되었습니다.  \n",
    "Expansive path에서는 각 블록에 2x2 up-convolution이 붙어 채널이 절반씩 줄어들고 특성 맵의 크기는 늘어납니다. Expansive path의 블록은 contracting block과 동일하게 3x3 convolution이 두 개씩 사용되었습니다.  \n",
    "두 Path에서 크기가 같은 블록의 출력과 입력은 skip connection처럼 연결해주어 low-level의 feature를 활용할 수 있도록 하였습니다. 마지막에는 1x1 convolution으로 원하는 시맨틱 세그멘테이션 맵을 얻을 수 있습니다.  \n",
    "\n",
    "결과적으로는, 입력으로 572x572 크기인 이미지가 들어가고 출력으로 388x388의 크기에 두 가지의 클래스를 가진 세그멘테이션 맵(segmentation map)이 나옵니다.  \n",
    "\n",
    "마지막 세그멘테이션 맵의 크기가 입력 이미지와 다른 것은 앞에서 이야기한 것처럼 세그멘테이션 맵을 원하는 크기로 조정하여(resize) 해결할 수 있습니다.  \n",
    "  \n",
    "Convolution은 padding을 통해서 크기를 같게 유지할 수 있으나, U-Net에선 padding을 하지않아서 deconvolution으로 확대하더라도 원래 이미지 크기가 될 수 없습니다. 그래서 입출력 값이 다릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-relative",
   "metadata": {},
   "source": [
    "#### 타일(Tile) 기법  \n",
    "![타일](https://aiffelstaticprd.blob.core.windows.net/media/images/unet.max-800x600.png)  \n",
    "FCN은 입력 이미지의 크기를 조정하여 세그멘테이션 맵을 얻어내는데, U-Net은 타일 기법을 사용하여 겹치는 구간을 타일로 나누어 네트워크를 추론, 큰 이미지에서도 높은 해상도의 세그멘테이션 맵을 얻을 수 있도록 했습니다. 그래서 해상도에서 차이를 냅니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-occasion",
   "metadata": {},
   "source": [
    "#### 데이터 불균형 해결  \n",
    "![불균형](https://aiffelstaticprd.blob.core.windows.net/media/images/unet_2.max-800x600.png)  \n",
    "세포를 검출해 내기 위해서는 세포들의 영역뿐만 아니라 경계를 예측을 해야하는데 이 것을 픽셀 단위로 라벨을 매긴다고 생각하면, 데이터셋에 세포나 배경보다는 절대적으로 세포 간 경계의 면적이 적을 수 밖에 없습니다. 클래스 간 데이터 양의 불균형을 해결해 주기 위해서 분포를 고려한 <b>weight map</b>을 학습 때 사용했다고 합니다.\n",
    "  \n",
    "weight는 손실함수(loss)에 적용되는 가중치를 말합니다. 의료 영상에서 세포 내부나 배경보다는 상대적으로 면적이 작은 세포 경계를 명확하게 추론해 내는 것이 더욱 중요하기 때문에, 세포 경계의 손실에 더 많은 페널티를 부과하는 방식입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-spice",
   "metadata": {},
   "source": [
    "### 주요 세그멘테이션 모델 (3) DeepLab 계열\n",
    "#### 참고자료\n",
    "- [Lunit 기술블로그의 DeepLab V3+: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation](https://blog.lunit.io/2018/07/02/deeplab-v3-encoder-decoder-with-atrous-separable-convolution-for-semantic-image-segmentation/)\n",
    "- [hyunjulie님의 2편: 두 접근의 접점, DeepLab V3+](https://medium.com/hyunjulie/2%ED%8E%B8-%EB%91%90-%EC%A0%91%EA%B7%BC%EC%9D%98-%EC%A0%91%EC%A0%90-deeplab-v3-ef7316d4209d)\n",
    "- [Taeoh Kim님의 PR-045: DeepLab: Semantic Image Segmentation](https://www.youtube.com/watch?v=JiC78rUF4iI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-sheep",
   "metadata": {},
   "source": [
    "![딥랩](https://aiffelstaticprd.blob.core.windows.net/media/images/deeplab_v3.max-800x600.png)  \n",
    "U-Net에서의 Contracting path과 Expansive path의 역할을 하는 것이 여기서는 위 그림의 인코더(Encoder), 디코더(Decoder)입니다.  \n",
    "  \n",
    "인코더는 이미지에서 필요한 정보를 특성으로 추출해내는 모듈이고 디코더는 추출된 특성을 이용해 원하는 정보를 예측하는 모듈입니다. 3x3 convolution을 사용했던 U-Net과 달리 DeepLabV3+는 Atrous Convolution을 사용하고 있습니다. 그리고 이로 Atrous Convolution을 여러 크기에 다양하게 적용한 것이 ASPP(Atrous Spatial Pyramid Pooling)입니다. DeepLab V3+는 ASPP가 있는 블록을 통해 특성을 추출하고 디코더에서 Upsampling을 통해 세그멘테이션 마스크를 얻고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-credits",
   "metadata": {},
   "source": [
    "#### Atrous Convolution  \n",
    "![Atrous](https://aiffelstaticprd.blob.core.windows.net/media/original_images/atrous_conv.gif)  \n",
    "- 더 넓은 영역을 보도록 해주기 위한 방법으로 커널이 일정 간격으로 떨어져 있습니다. 이를 통해 컨볼루션 레이어를 너무 깊게 쌓지 않아도 넓은 영역의 정보를 커버할 수 있게 됩니다.\n",
    "- [ 딥러닝에서 사용되는 여러 유형의 Convolution 소개](https://zzsza.github.io/data/2018/02/23/introduction-convolution/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-poison",
   "metadata": {},
   "source": [
    "#### Spatial Pyramid Pooling  \n",
    "![spatial](https://aiffelstaticprd.blob.core.windows.net/media/images/GC-5-L-SPP.max-800x600.png)  \n",
    "- 여러 가지 스케일로 convolution과 pooling을 하고 나온 다양한 특성을 연결(concatenate)해 줍니다. 이를 통해서 멀티스케일로 특성을 추출하는 것을 병렬로 수행하는 효과를 얻을 수 있습니다. 여기서 컨볼루션을 Atrous Convolution으로 바꾸어 적용한 것은 Atrous Spatial Pyramid Pooling이라고 합니다. 이러한 아키텍쳐는 입력이미지의 크기와 관계없이 동일한 구조를 활용할 수 있다는 장점이 있습니다. 그러므로 제각기 다양한 크기와 비율을 가진 RoI 영역에 대해 적용하기에 유리합니다.\n",
    "- [갈아먹는 Object Detection - Spatial Pyramid Pooling Network](https://yeomko.tistory.com/14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-supervisor",
   "metadata": {},
   "source": [
    "### 세그멘테이션의 평가\n",
    "- [ Evaluating image segmentation models](https://www.jeremyjordan.me/evaluating-image-segmentation-models/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-baltimore",
   "metadata": {},
   "source": [
    "#### 1) 픽셀별 정확도 (Pixel Accruacy)  \n",
    "![정확도](https://aiffelstaticprd.blob.core.windows.net/media/images/error_metric.max-800x600.jpg)  \n",
    "- 예측 결과 맵(prediction map)을 클래스 별로 평가하는 경우에 이진 분류 문제(binary classification)로 생각해 픽셀 및 채널 별로 평가합니다. \n",
    "  픽셀 별 이미지 분류 문제로 평가하는 경우에는 픽셀 별로 정답 클래스를 맞추었는지 여부, 즉 True/False를 구분합니다.\n",
    "\n",
    "- 4x4의 크기를 가지는 map에서 중앙의 2x2의 영역이 전경이고 예측 결과 중 한 칸을 놓쳤다면 (TP+TN)/(FP+FN+TP+TN)으로 Accuracy를 구할 수 있습니다. TP(True positive)+ TN(True negative)는 옳게 분류된 샘플의 수로 잘못 예측된 한 칸을 제외한 15입니다. \n",
    "  그리고 False case는 1인 한칸은 전경이 배경으로 예측되었으니 FN(False negative)입니다. 따라서 분모항은 16이 됩니다. 따라서 Pixel Accruacy는 15/16으로 계산할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-vector",
   "metadata": {},
   "source": [
    "#### 2) 마스크 IoU (Mask Intersection-over-Union)  \n",
    "- 물체 검출 모델을 평가할 때는 정답 라벨(ground truth)와 예측 결과 바운딩 박스(prediction bounding box) 사이의 IoU(intersection over union)를 사용합니다. \n",
    "  마스크도 일종의 영역임을 생각했을 때 세그멘테이션 문제에서는 정답인 영역과 예측한 영역의 IoU를 계산할 수 있을 것입니다.\n",
    "```\n",
    "# sample for mask iou\n",
    "intersection = np.logical_and(target, prediction)\n",
    "union = np.logical_or(target, prediction)\n",
    "iou_score = np.sum(intersection) / np.sum(union)\n",
    "```\n",
    "- 마스크 IoU를 클래스 별로 계산하면 한 이미지에서 여러 클래스에 대한 IoU 점수를 얻을 수 있습니다. 이를 평균하면 전체적인 시맨틱 세그멘테이션 성능을 가늠할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-diameter",
   "metadata": {},
   "source": [
    "### Upsampling의 다양한 방법\n",
    "#### 1) Nearest Neighbor  \n",
    "![NN](https://aiffelstaticprd.blob.core.windows.net/media/original_images/upsampling1.png)  \n",
    "- Nearest upsampling은 scale을 키운 위치에서 원본에서 가장 가까운 값을 그대로 적용하는 방법\n",
    "- 2x2 matrix가 있을때 이를 2배로 키우면 4x4의 matrix가 됨\n",
    "  좌측 상단으로부터 2x2는 입력 matrix의 1x1과 가장 가까우므로 해당 값을 그대로 사용하게 됨\n",
    "  \n",
    "#### 2) Bilinear Interpolation  \n",
    "![Bi](https://aiffelstaticprd.blob.core.windows.net/media/original_images/bi_interpolation.png)  \n",
    "- 두 축에 대해서 선형보간법을 통해 필요한 값을 메우는 방식\n",
    "- 2x2 matrix를 4x4로 upsampling을 할 때 위의 이미지 처럼 빈 값을을 채워야할때, 선형보간법(Linear interpolation)을 사용하게 되는데 축을 두 방향으 로 활용하기 때문에 Bilinear interpolation이라고 함\n",
    "- [bskyvision의 선형보간법(linear interpolation)과 삼차보간법(cubic interpolation), 제대로 이해하자](https://bskyvision.com/m/789)\n",
    "  \n",
    "#### 3) Transposed Convolution  \n",
    "![TC](https://aiffelstaticprd.blob.core.windows.net/media/images/transposed_conv.max-800x600.jpg)  \n",
    "- Convolution Layer는 Kernel의 크기를 정의하고 입력된 Feature를 Window에 따라서 output을 계산하는데 Transposed Convolution은 이와 반대의 연산을하여 거꾸로 학습된 파라미터로 입력된 벡터를 통해 더 넓은 영역의 값을 추정해냄\n",
    "- [ Up-sampling with Transposed Convolution](https://zzsza.github.io/data/2018/06/25/upsampling-with-transposed-convolution/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
